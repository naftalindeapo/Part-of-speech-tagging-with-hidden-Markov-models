{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed38baf1",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging with hidden Markov models\n",
    "### *by Naftali N Indongo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b19218cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from unidecode import unidecode\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afde5ba",
   "metadata": {},
   "source": [
    "### (1.) Data\n",
    "\n",
    "In this assignment we will use the $\\verb|NCHLT|$ $\\verb|Afrikaans|$ $\\verb|Annotated|$ $\\verb|Text|$ $\\verb|Corpora|$ downloaded from the Resource Catalogue of the South African Centre for Digital Language Resources (SADiLAR) found [here](https://repo.sadilar.org).We will be using the excel files for training and test found the in directory $\\verb|2|$. $\\verb|POS|$ $\\verb|Annotated|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41939003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Laai</td>\n",
       "      <td>VTHOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>die</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elektroniese</td>\n",
       "      <td>ASA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aansoekvorm</td>\n",
       "      <td>NSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>af</td>\n",
       "      <td>UPW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Token    POS\n",
       "0          Laai  VTHOG\n",
       "1           die     LB\n",
       "2  elektroniese    ASA\n",
       "3   aansoekvorm    NSE\n",
       "4            af    UPW"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the datasets\n",
    "af_test = pd.read_excel('Data/GOV-ZA.5000TestSet.af.pos.full.xls')\n",
    "af_train = pd.read_excel('Data/GOV-ZA.50000TrainingSet.af.pos.full.xls')\n",
    "af_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c1d0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'.' in af_train['Token'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4d0f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6285</th>\n",
       "      <td>http://www.cites.org</td>\n",
       "      <td>NEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6758</th>\n",
       "      <td>http://www.cites.org</td>\n",
       "      <td>NEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6882</th>\n",
       "      <td>http://www.mcm-deat.gov.za</td>\n",
       "      <td>NEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7415</th>\n",
       "      <td>http://www.cites.org</td>\n",
       "      <td>NEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7555</th>\n",
       "      <td>http://www.mcm-deat.gov.za</td>\n",
       "      <td>NEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57429</th>\n",
       "      <td>http://www.services.gov.za</td>\n",
       "      <td>NEE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Token  POS\n",
       "6285         http://www.cites.org  NEE\n",
       "6758         http://www.cites.org  NEE\n",
       "6882   http://www.mcm-deat.gov.za  NEE\n",
       "7415         http://www.cites.org  NEE\n",
       "7555   http://www.mcm-deat.gov.za  NEE\n",
       "57429  http://www.services.gov.za  NEE"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a regular expression pattern to match website URLs\n",
    "website_pattern = r'^https?://(?:www\\.)?[\\w-]+\\.[\\w.-]+[\\w/]*$'\n",
    "\n",
    "# Use the `str.contains` method to check if the Token column contains website URLs\n",
    "website_instances = af_train[af_train['Token'].str.contains(website_pattern, case=False, na=False)]\n",
    "website_instances\n",
    "# af_train[af_train['POS']=='BOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5b1d00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Die</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doel</td>\n",
       "      <td>NSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>van</td>\n",
       "      <td>SVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>die</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>webtuiste</td>\n",
       "      <td>NSE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Token  POS\n",
       "0        Die   LB\n",
       "1       doel  NSE\n",
       "2        van  SVS\n",
       "3        die   LB\n",
       "4  webtuiste  NSE"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "af_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073cccf4",
   "metadata": {},
   "source": [
    "#### (2.) Data Normalization and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8121702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the last row\n",
    "af_test = af_test[:-1]\n",
    "af_train = af_train[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a1ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Begin_n_End_sentence(dataset):\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for i in range(len(dataset)):\n",
    "        # Check if both Token and POS are NaN\n",
    "        if pd.isna(dataset.at[i, 'Token']) and pd.isna(dataset.at[i, 'POS']):\n",
    "            # Replace Token and POS with 'START' and '<s>'\n",
    "            dataset.at[i, 'Token'] = 'START'\n",
    "            dataset.at[i, 'POS'] = '<s>'\n",
    "\n",
    "            # Check if there is a previous row\n",
    "            if i > 0:\n",
    "                # Replace Token and POS in the previous row with '</s>' and 'EOS'\n",
    "                dataset.at[i-1, 'Token'] = 'END'\n",
    "                dataset.at[i-1, 'POS'] = '</s>'\n",
    "    \n",
    "    # Return the updated dataset\n",
    "    return dataset\n",
    "\n",
    "A = Begin_n_End_sentence(af_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bb48bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the data\n",
    "def preprocess_data(data):\n",
    "    \n",
    "    #Modelling the beginning of the first setentence by inserting a new row with Token = '<s>' and POS = 'BOS'\n",
    "    first_row = pd.DataFrame({'Token': ['START'], 'POS': ['<s>']})\n",
    "\n",
    "    # Concatenate the new row with the original DataFrame\n",
    "    data = pd.concat([first_row, data]).reset_index(drop=True)\n",
    "    \n",
    "    #Model the beginning and end of sentence\n",
    "    data = Begin_n_End_sentence(data)\n",
    "    \n",
    "    # Convert the 'Token' column to lowercase\n",
    "    data['Token'] = data['Token'].apply(lambda x: x.lower() if x not in ['START', 'END'] else x)\n",
    "\n",
    "\n",
    "    # Remove punctuation from the Token column, excluding '<s>' and '</s>'\n",
    "    data['Token'] = data['Token'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x) \\\n",
    "                                          if isinstance(x, str) and x != '</s>' and x!='<s>' else x)\n",
    "    # Convert diacritics to closest ASCII representation in the Token column\n",
    "    data['Token'] = data['Token'].apply(lambda x: unidecode(x) if isinstance(x, str) else x)\n",
    "    # Drop the rows where the value in the 'Token' column is a space (' ')\n",
    "    data = data[data['Token'] != '']\n",
    "    \n",
    "    #Modelling the end of the last setentence by inserting a new row with Token = '</s>' and POS = 'EOS'\n",
    "    last_row = pd.DataFrame({'Token': ['END'], 'POS': ['</s>']})\n",
    "\n",
    "    # Concatenate the new row with the original DataFrame\n",
    "    data = pd.concat([data, last_row]).reset_index(drop=True)\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Reset the row indices\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7bbbad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "af_test = preprocess_data(af_test)\n",
    "af_train = preprocess_data(af_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b9fb2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>START</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>laai</td>\n",
       "      <td>VTHOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>die</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elektroniese</td>\n",
       "      <td>ASA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aansoekvorm</td>\n",
       "      <td>NSE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Token    POS\n",
       "0         START    <s>\n",
       "1          laai  VTHOG\n",
       "2           die     LB\n",
       "3  elektroniese    ASA\n",
       "4   aansoekvorm    NSE"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "af_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdbe9b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token    0\n",
      "POS      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the entire DataFrame\n",
    "missing_values = af_train.isnull().sum()\n",
    "\n",
    "# Print the number of missing values for each column\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c20b44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5693</th>\n",
       "      <td>ons</td>\n",
       "      <td>PEMB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5694</th>\n",
       "      <td>land</td>\n",
       "      <td>NSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5695</th>\n",
       "      <td>te</td>\n",
       "      <td>UPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5696</th>\n",
       "      <td>maak</td>\n",
       "      <td>VTHOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5697</th>\n",
       "      <td>END</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Token    POS\n",
       "5693   ons   PEMB\n",
       "5694  land    NSE\n",
       "5695    te    UPI\n",
       "5696  maak  VTHOG\n",
       "5697   END   </s>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "af_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0af41418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and validation\n",
    "Train_set = af_train.iloc[:39064]\n",
    "Val_set = af_train.iloc[39064:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "407eca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of training, validation and test tagged words\n",
    "Train_tagged_words = [(row[\"Token\"], row[\"POS\"]) for _, row in Train_set.iterrows()]\n",
    "Val_tagged_words = [(row[\"Token\"], row[\"POS\"]) for _, row in Val_set.iterrows()]\n",
    "Test_tagged_words = [(row[\"Token\"], row[\"POS\"]) for _, row in af_test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98e0c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many unique tags are present in training data\n",
    "Unique_tags = [tag for word,tag in Train_tagged_words]\n",
    " \n",
    "# check total words in the training vocabulary\n",
    "vocab = {word for word,tag in Train_tagged_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bcecb5",
   "metadata": {},
   "source": [
    "#### 3. Converting each dataset into a list of lists of tupples (word, tag) and splitting the training set into training and validation\n",
    "\n",
    "Below we will convert the training and test sets into lists of lists of tupples $\\verb|(word, tag)|$, where each inner list corresponds to a sentence. We will also split the training set into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7633ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_boundaries(dataset):\n",
    "    sentence_boundaries = []  # List to store the indices where sentences end\n",
    "\n",
    "    # Iterate over the rows of the dataset\n",
    "    for i in range(len(dataset)):\n",
    "        # Check if the Token is 'END'\n",
    "        if dataset.at[i, 'Token'] == 'END':\n",
    "            sentence_boundaries.append(i+1)  # Append the index of the next row\n",
    "\n",
    "    # Return the list of sentence boundaries\n",
    "    return sentence_boundaries\n",
    "\n",
    "def convert_dataset(dataset, sentence_boundaries):\n",
    "    sentences = []  # List to store the converted dataset\n",
    "\n",
    "    start_index = 0  # Start index of the current sentence\n",
    "\n",
    "    # Iterate over the sentence boundaries\n",
    "    for end_index in sentence_boundaries:\n",
    "        sentence = []  # List to store the tuples of (token, tag) for the current sentence\n",
    "\n",
    "        # Iterate over the rows within the current sentence boundaries\n",
    "        for i in range(start_index, end_index):\n",
    "            token = dataset.at[i, 'Token']\n",
    "            tag = dataset.at[i, 'POS']\n",
    "            sentence.append((token, tag))\n",
    "\n",
    "        sentences.append(sentence)  # Append the sentence to the list of sentences\n",
    "        start_index = end_index  # Update the start index for the next sentence\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e6a1cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2613"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_indices = create_sentence_boundaries(af_test)\n",
    "Train_indices = create_sentence_boundaries(af_train)\n",
    "af_test_list = convert_dataset(af_test, Test_indices)\n",
    "af_train_list = convert_dataset(af_train, Train_indices)\n",
    "len(af_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab33f7b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Splitting into training and validation, and converting to arrays.\n",
    "Train_data = af_train_list[:2090]\n",
    "Val_data = af_train_list[2090:]\n",
    "Test_data = af_test_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b37a96",
   "metadata": {},
   "source": [
    "### Part 1: Developing a basic HMM tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307690b2",
   "metadata": {},
   "source": [
    "#### (a) Dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1eb974c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39064"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of tupples (token, tag) for the datasets\n",
    "Train_word_tag = [ tup for sent in Train_data for tup in sent ]\n",
    "Val_word_tag = [ tup for sent in Val_data for tup in sent]\n",
    "Test_word_words = [ tup for sent in Test_data for tup in sent]\n",
    "\n",
    "#Getting the tags and tagged words\n",
    "Training_tags = [tag for word,tag in Train_word_tag]\n",
    "Train_tagged_words = [word for word,tag in Train_word_tag]\n",
    "len(Train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d49b1e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 95 training tags.\n",
      "There are 4610 unique word tokens in the training set\n"
     ]
    }
   ],
   "source": [
    "# Checking how many unique tags are present in training data\n",
    "Unique_tags = {tag for word,tag in Train_word_tag}\n",
    "print('There are {} training tags.'.format(len(Unique_tags)))\n",
    " \n",
    "# check the unique word tokens in the training vocabulary\n",
    "Training_vocab = {word for word,tag in Train_word_tag}\n",
    "print('There are {} unique word tokens in the training set'.format(len(Training_vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f496ea4",
   "metadata": {},
   "source": [
    "#### (b) Model splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98736b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting the n-grams\n",
    "def ngrams(sentence, n):\n",
    "    Ngrams = []\n",
    "    for i in range(len(sentence)):\n",
    "        Ngrams.append(tuple(sentence[i: i + n]))\n",
    "    return Ngrams\n",
    "\n",
    "# function for getting bigram count\n",
    "def bigram_counts(tags):\n",
    "    bigram_cnt = {}\n",
    "    for i_tag_bigram in ngrams(tags, 2):\n",
    "        if i_tag_bigram in bigram_cnt:\n",
    "            bigram_cnt[i_tag_bigram] += 1\n",
    "        else:\n",
    "            bigram_cnt[i_tag_bigram] = 1\n",
    "    return bigram_cnt\n",
    "\n",
    "#function for getting unigram count\n",
    "def unigram_counts(tags):\n",
    "    unigram_cnt = {}\n",
    "    for tag in tags:\n",
    "        if tag in unigram_cnt:\n",
    "            unigram_cnt[tag] += 1\n",
    "        else:\n",
    "            unigram_cnt[tag] = 1\n",
    "    return unigram_cnt\n",
    "\n",
    "\n",
    "#function for getting tagged word count\n",
    "def tag_word_counts(tagged_words):\n",
    "    tag_count = defaultdict(lambda: 0)\n",
    "    tag_word_count = Counter()\n",
    "    for word, tag in tagged_words:\n",
    "        tag_count[tag] += 1\n",
    "        if (word, tag) in tag_word_count:\n",
    "            tag_word_count[(word, tag)] += 1\n",
    "        else:\n",
    "            tag_word_count[(word, tag)] = 1\n",
    "    return tag_count, tag_word_count\n",
    "\n",
    "# Estimating the transition probabilities\n",
    "def transition_probabilty(tags, bigram_cnt, unigram_cnt):\n",
    "    transition_probabilities = defaultdict(lambda: 0)\n",
    "    bigrams = ngrams(tags, 2)\n",
    "    for bigram in bigrams:\n",
    "        transition_probabilities[bigram] = bigram_cnt[bigram] / unigram_cnt[bigram[0]]\n",
    "    return transition_probabilities\n",
    "\n",
    "# Estimate the emmission probabilities \n",
    "def emmission_probabilty(tag_word_count,tagged_words, tag_count):\n",
    "    emmission_probabilities = defaultdict(lambda: 0)\n",
    "    for word, tag in tagged_words:\n",
    "        emmission_probabilities[(word, tag)] = tag_word_count[(word, tag)] / tag_count[tag]\n",
    "    return emmission_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae8b9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Estimating the transition probabilities############\n",
    "# 1. Calculate the bigram counts C(tag_t,tag_{t-1})\n",
    "Bigram_counts = bigram_counts(Training_tags)\n",
    "\n",
    "# 2. Calculate the unigram counts C(tag_{t-1})\n",
    "Unigram_counts = unigram_counts(Training_tags)\n",
    "\n",
    "# 3. Compute the transition probabilities\n",
    "Transition_probs = transition_probabilty(Training_tags, Bigram_counts, Unigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f768fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5026\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "######### Estimating the emission probabilities############\n",
    "# 1. Calculate the tag_counts and tag_word counts C(tag_t) and C(word_t,tag_t})\n",
    "tag_counts, tag_word_counts = tag_word_counts(Train_word_tag)\n",
    "print(len(tag_word_counts))\n",
    "print(len(tag_counts))\n",
    "\n",
    "# 2. Compute the transition probabilities\n",
    "Emmission_Probs = emmission_probabilty(tag_word_counts,Train_word_tag, tag_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f274c9",
   "metadata": {},
   "source": [
    "#### (c) The Viterbi algorithm for tagging new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5bb8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoder(sentence, transition_probs, emission_probs):\n",
    "    observable = [word for word, tag in sentence]\n",
    "    in_states = [tag for word, tag in sentence]\n",
    "    states = in_states\n",
    "    K = len(states)\n",
    "    deltas = {}\n",
    "    phi_s = {}\n",
    "    \n",
    "    unk_prob = 0.00001\n",
    "    start_tag = '<s>'\n",
    "    end_tag = '</s>'\n",
    "    \n",
    "    # Initialization\n",
    "    for j, state in enumerate(states):\n",
    "        deltas[state, 1] = emission_probs.get((observable[1], state), unk_prob) * transition_probs.get((start_tag, state), unk_prob)\n",
    "\n",
    "    # Recursion\n",
    "    for t in range(2, len(observable)):\n",
    "        obs = observable[t]\n",
    "        for j, state in enumerate(states):\n",
    "            max_prob = 0.0\n",
    "            max_index = 0\n",
    "            for i, prev_state in enumerate(states):\n",
    "                prob = deltas[prev_state, t-1] * transition_probs.get((prev_state, state), unk_prob) * emission_probs.get((obs, state), unk_prob)\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_index = i\n",
    "            deltas[state, t] = max_prob\n",
    "            phi_s[state, t] = max_index\n",
    "\n",
    "    # Termination\n",
    "    max_prob = 0.0\n",
    "    max_index = 0\n",
    "    for j, state in enumerate(states):\n",
    "        prob = transition_probs.get((state, end_tag), unk_prob) * deltas[state, len(observable) - 1]\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_index = j\n",
    "\n",
    "    # Backtracking\n",
    "    best_path = []\n",
    "    best_path.append(start_tag)  # Add the start tag at the beginning\n",
    "    best_path.append(states[max_index])\n",
    "    for t in range(len(observable) - 1, 1, -1):\n",
    "        max_index = phi_s[states[max_index], t]\n",
    "        best_path.insert(1, states[max_index])  # Insert the tag at the second position to maintain the order\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "\n",
    "#Test\n",
    "Test_results = viterbi_decoder(Val_data[0], Transition_probs, Emmission_Probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fe40226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Actual_tags</th>\n",
       "      <th>Predicted_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>START</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>verstrek</td>\n",
       "      <td>VTHOG</td>\n",
       "      <td>VTHOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>die</td>\n",
       "      <td>LB</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in</td>\n",
       "      <td>SVS</td>\n",
       "      <td>SVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>die</td>\n",
       "      <td>LB</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>afdeling</td>\n",
       "      <td>NSE</td>\n",
       "      <td>NSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>manufacturer</td>\n",
       "      <td>RV</td>\n",
       "      <td>SVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>for</td>\n",
       "      <td>RV</td>\n",
       "      <td>RV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>agoa</td>\n",
       "      <td>RK</td>\n",
       "      <td>RV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>particulars</td>\n",
       "      <td>RV</td>\n",
       "      <td>RV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>END</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Token Actual_tags Predicted_tags\n",
       "0          START         <s>            <s>\n",
       "1       verstrek       VTHOG          VTHOG\n",
       "2            die          LB             LB\n",
       "3             in         SVS            SVS\n",
       "4            die          LB             LB\n",
       "5       afdeling         NSE            NSE\n",
       "6   manufacturer          RV            SVS\n",
       "7            for          RV             RV\n",
       "8           agoa          RK             RV\n",
       "9    particulars          RV             RV\n",
       "10           END        </s>           </s>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some examples:\n",
    "Sentence = [('START', '<s>'), ('verstrek', 'VTHOG'), ('die', 'LB'), ('in', 'SVS'), ('die', 'LB'), ('afdeling', 'NSE'),\\\n",
    "     ('manufacturer', 'RV'), ('for', 'RV'), ('agoa', 'RK'), ('particulars', 'RV'), ('END', '</s>')]\n",
    "\n",
    "# Get the actual tags and predicted tags\n",
    "Actual_tags = [tag for word,tag in Sentence]\n",
    "Words = [word for word, tag in Sentence]\n",
    "\n",
    "# Get the predicted tags\n",
    "Predicted_tags = viterbi_decoder(Sentence, Transition_probs, Emmission_Probs)\n",
    "\n",
    "Results_table = pd.DataFrame({'Token': Words, 'Actual_tags': Actual_tags, 'Predicted_tags': Predicted_tags})\n",
    "Results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0acb696",
   "metadata": {},
   "source": [
    "#### (d.) Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d732efbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9062829062829063\n"
     ]
    }
   ],
   "source": [
    "#function for calculating the accuracy on a given set\n",
    "def calculate_accuracy(given_set, transition_probs, emission_probs):\n",
    "    correct_tags = 0\n",
    "    total_tags = 0\n",
    "    \n",
    "    for sentence in given_set:\n",
    "        predicted_tags = viterbi_decoder(sentence, transition_probs, emission_probs)\n",
    "        actual_tags = [tag for _, tag in sentence]\n",
    "        \n",
    "        for predicted_tag, actual_tag in zip(predicted_tags, actual_tags):\n",
    "            if predicted_tag == actual_tag:\n",
    "                correct_tags += 1\n",
    "            total_tags += 1\n",
    "    \n",
    "    accuracy = correct_tags / total_tags\n",
    "    return accuracy\n",
    "\n",
    "# Assuming you have a validation_set containing sentences in the same format as your dataset\n",
    "accuracy = calculate_accuracy(Test_data, Transition_probs, Emmission_Probs)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c68f59c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the validation set is 90.7528%\n",
      "The accuracy on the test set is 90.6283%\n"
     ]
    }
   ],
   "source": [
    "############# Calculating the accuracy of our HMM tagger on the validation and test set #########\n",
    "Val_accuracy =  calculate_accuracy(Val_data, Transition_probs, Emmission_Probs)\n",
    "Test_accuracy = calculate_accuracy(Test_data, Transition_probs, Emmission_Probs)\n",
    "\n",
    "print(f'The accuracy on the validation set is {Val_accuracy:.4%}')\n",
    "print(f'The accuracy on the test set is {Test_accuracy:.4%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11c1ed",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
